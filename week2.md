# Week2
`J009_강성재` `J059_김윤서` `J068_김준서` `S016_서준영`

## Week1 퀘스트

| 퀘스트 번호 | 내용                                                                         |
| ----------- | ---------------------------------------------------------------------------- |
| Quest 1     | 매일 채점 프롬프트를 이용해, 피어세션 문서의 부족한 부분을 AI에게 피드백받기 |
| Quest 2     | 매일 채점 프롬프트를 조금씩 개선하며 더 나은 피드백을 받을 수 있게 만들기    |
| Quest 3     | 피어세션 전에 AI에게 “이 자료에 대해 어떤 질문을 받을 수 있을까?”를 물어보기 |
| Quest 4     | 나의 루틴을 AI에게 공유하고, 정해진 시간마다 알림을 받아 실천 여부 점검하기  |

### Week1 퀘스트 수행 기록

### `J094 노주호`
1. **`선택한 퀘스트`**
    - `Quest 3. AI에게 더 나은 질문 만들기`
2. **`개선할 사항: -`**

### `J260 지은미`
1. **`선택한 퀘스트`**
    - `Quest 1. 피어세션용 문서를 AI로 채점받기`
    - `Quest 3. AI에게 더 나은 질문 만들기`
2. **`개선할 사항: 정리하는 부분에서는 질문의 퀄리티나 정리하는 방식을 조금 더 학습시켜 실험해봐야겠다는 생각을 했다. 아쉬웠던건 지피티 특유의 정리 방식으로 정리를 진행해줘서 내가 정리한 느낌이 안난다는 아쉬움이 있었다. 내가 정리하는 방식에 대해 학습을 시켜놓은 뒤, 정리를 부탁하면 더 만족하는 정리본을 얻을 수 있을 것 같다.`**

### `J017 곽윤철`
1. **`선택한 퀘스트`**
    - `Quest 1. 피어세션용 문서를 AI로 채점받기`
2. **`개선할 사항: -`**

### `S005_김성훈`
1. **`선택한 퀘스트`**
    - `Quest 3. AI에게 더 나은 질문 만들기`
2. **`개선할 사항: "무엇을 알고 싶은지"뿐만 아니라 "왜, 어떤 상황에서" 궁금한지도 함께 담는 것이 중요하다`**

## Week2 퀘스트

| 퀘스트 번호 | 내용                                                               |
| ----------- |------------------------------------------------------------------|
| Quest 1     | 채점 프롬프트를 이용해, 피어세션 문서의 부족한 부분을 AI에게 피드백받고, 피드백 내용을 참고해 보완하고 개선하기 |
| Quest 2     | 칭찬과 피드백 포인트를 미리 기록해 둘 수 있는 피어세션 템플릿 만들기                  |
| Quest 3     | 피어세션 문서와 예상 질문들을 기반으로 보다 예리하고 유의미한 질문 생성해보기                      |
| Quest 4     | 일정과 루틴에 맞춰 알림 보내주는 슬랙 봇 만들어보기                                    |

## 퀘스트 개선 과정

### 토론
`J059_김윤서` `J068_김준서`
- 피어 세션 피드백 경험 공유
    - 1주차는 서먹함/막막함 등으로 표면적 피드백과 의견 공유 수준
    - 2주차는 약간의 익숙해짐과 학습 경험을 통해 비교적 세부적인 피드백 진행,  
      나름의 기준과 방식을 지정해 진행했으나, 심층적 피드백을 통한 유의미한 결과물 산출은 미미
    - 보다 유의미하고 생산적인 피드백 필요성 공감
    - 해당 내용 반영해 퀘스트 보완 및 수정

### Quest 1  
before | after  
|--------|--------|  
| 매일 채점 프롬프트를 이용해, 피어세션 문서의 부족한 부분을 AI에게 피드백받기 | 채점 프롬프트를 이용해, 피어세션 문서의 부족한 부분을 AI에게 피드백받고, 피드백 내용을 참고해 보완하고 개선하기 |  

<br />

#### ✏️ 수정된 이유  
- 기존에는 AI 채점만 받고 끝나는 경우가 많았음  
- 프롬프트 실험은 했지만, 그 결과를 반영한 문서 개선까지 이어지지 않아 아쉬움
- 일부 참가자 피드백처럼, GPT의 일관된 정리 방식이 아쉬웠고  
  → 내가 원하는 정리 스타일을 반영하거나, 직접 수정하는 과정이 필요했음  

<br />

#### 💬 토론 내용  
- 1주차 피어세션 질문은 어색하거나 쥐어짠 느낌이 많았고  
  피드백을 받아도 질문 개선으로 이어지지 않는 경우가 대부분
- 퀘스트 1, 2가 분리되어 있어 루틴이 흐지부지될 수 있다는 피드백
- “그냥 피드백만 받는 게 아니라, 받고 나서 어떻게 바꿨는지까지 정리해야 의미가 있다”는 의견 다수  

<br />

#### 🔍 조사 활동  
- 실제로 다양한 프롬프트 실험 진행  
  - 같은 질문을 더 구체적으로 표현했을 때, AI 피드백이 어떻게 달라지는지 비교  
  - “질문 설계 배경”을 추가하면 더 명확한 개선점이 나오는 것을 확인  
- 일부 참가자는 AI가 주는 정리 스타일이 아쉽다고 응답  
  → GPT에게 내가 원하는 정리 스타일을 먼저 학습시키고 나서 피드백을 요청하는 방식도 시도  

<br /> <br />

### Quest 2  
| before | after |
|--------|-------|
| --- | 칭찬과 피드백 포인트를 미리 기록해 둘 수 있는 피어세션 템플릿 만들기 |

<br />

#### ✏️ 수정된 이유
- 피어세션에서 피드백을 즉흥적으로 주고 끝나는 경우가 많았음  
- 미리 칭찬할 점과 아쉬운 점을 적어두면, 피드백이 더 구체적이고 풍부해질 수 있음  
- 처음부터 정해진 양식은 없었지만, 서로에게 도움이 되는 문서를 만들어보자는 취지로 시작

<br />

#### 💬 토론 내용
- 질문이 어색하다는 피드백이 있었고, 왜 그런 질문을 했는지 보이지 않는다는 의견이 많았음  
- 피어세션 전에 좋았던 점과 피드백 포인트를 미리 생각해두면 더 깊이 있는 대화가 가능하다는 공감대  
- AI 피드백과 사람 피드백을 모두 담을 수 있는 간단하고 명확한 템플릿이 있으면 좋겠다는 결론
  
<br />

#### 🔍 조사 활동
- 기존에 그런 문서는 없었지만, 어떻게 하면 서로 피드백을 잘 주고받을 수 있을까를 중심으로 논의  
- 몇 가지 항목(칭찬할 점, 개선할 점, 질문 의도 등)을 정리해서 템플릿 형태로 만들어보기로 함
- 실제로 작성해보니 피드백이 더 명확하고 전달이 쉬워짐을 느꼈음

### Quest 3
before | after  
|--------|--------|  
| 피어세션 전에 AI에게 “이 자료에 대해 어떤 질문을 받을 수 있을까?”를 물어보기 | 피어세션 문서와 예상 질문들을 기반으로 보다 예리하고 유의미한 질문 생성해보기 |  

<br />

#### ✏️ 수정된 이유
- 예상 질문을 일방적으로 수용만 해선 유의미한 개선점을 파악하고 보완할 수 없음
- 피어 피드백 과정에서 주고받을 질문이 부족하거나, 얕은 질문으로 금방 마무리되는 경우 다수
- 작성한 코드에 대한 심층적 이해 부족을 스스로 개선해나갈 필요성 자각

<br />

#### 💬 토론 내용
- 피어 피드백을 진행할 때 질문이 겉돌거나 유의미한 결과를 내지 못하는 경우가 많음
- 받을 질문을 선제적으로 받아 버리니 자주적인 학습이 차단되는 느낌

<br />

#### 🔍 조사 활동
- 최근 미션(day09) 학습 정리를 기반으로,  
  직접 구상한 질문과 피어 피드백 과정에서 주고받은 질문을 입력,
  - `각각의 문서에 정리된 내용을 기반으로 어떤 질문을 할 수 있을까?`
  - `좀더 개선되거나 핵심적인 내용을 묻는 질문으로 개선할 수 있을까?`
- ### 개선안 : 설계와 반복
    - 기존 질문 / 개선 질문 제안 생성, 질문에 대한 질문을 통해 본질적인 내용을 파악하기
    - 표면적 기능 설명 묻기→"왜 꼭 그 구조인가? 현실에서 부작용·트레이드오프는?" 등으로  
      맥락을 설계와 실무, 성능, 품질에 끊임없이 연결하기
    - 보편적인 컨셉(불변성, 싱글톤, 동기화 등)과 설계 패턴이 **자신의 문제에 정말 맞는가? 충분한가?** 를  
      반복해서 물으며, 대체 구조·한계·사용 맥락을 탐구하는 것이 중요하다

<br /> <br />

### Quest 4
before | after  
|--------|--------|  
| 나의 루틴을 AI에게 공유하고, 정해진 시간마다 알림을 받아 실천 여부 점검하기 | 일정과 루틴에 맞춰 알림 보내주는 슬랙 봇 만들어보기 |  

<br />

#### ✏️ 수정된 이유
- 기존 미션 `나의 루틴을 AI에게 공유하고, 정해진 시간마다 알림을 받아 실천 여부 점검하기`  
  은 대부분의 모델이 알림 전송 기능을 제공하지 않으므로 실천 불가.
- 대부분의 모델은 직접적 알림 전송은 불가하지만 해당 기능을 제공하는 프로그램 생성을 권유.
- 코어 타임 동안 온-라인 해야 하는 슬랙에 봇 추가하는 방식으로 우회.

<br />

#### 💬 토론 내용
- 각자의 일정 진행 방식과 루틴이 존재하고,  
  일정 알림 기능의 필요성은 공감하나 해당 기능을 제공하는 모델은 거의 없음
- 다만 슬랙 봇을 통해 동일 기능을 제공은 가능 like booduki
- 자신만의 슬랙봇 만들어보기 퀘스트로 수정.

<br />

#### 🔍 조사 활동
- 슬랙 봇 제작 방식

| 단계               | 설명                                               |
|:------------------:|:--------------------------------------------------|
| 워크스페이스 준비   | 슬랙 워크스페이스 생성 또는 기존 워크스페이스 참여 |
| 앱/봇 생성         | Slack API에서 앱 및 봇 계정 추가, 권한 세팅        |
| 토큰 발급          | 앱 설치 후 Bot Token(시크릿 키) 발급              |
| 개발 및 테스트      | 봇 코드 작성 및 기능 테스트                       |
| 배포               | 서버·클라우드 등에 배포하여 실서비스로 운영        |

- ### 개선안 : 자신만의 슬랙봇
  - 스스로 슬랙봇을 만들어보는 경험을 넘어, 자신만의 커스터마이징을 통한 
  - 사용자 맞춤형 봇 만들고 활용해 보기

- ## 정리
  - 보다 실천 가능한, 프로젝트의 목표에 부합한 퀘스트로의 개선을 통해
  - 더욱 효과적인, 의미있는 피드백을 진행하고 지속 가능한 성장을 추구하기

<br /> <br />

## 27조의 멤버가 선택한 퀘스트

### `S001 강윤서`
1. **`선택한 퀘스트`**
    - `Quest 2. 칭찬과 피드백 포인트를 미리 기록해 둘 수 있는 피어세션 템플릿 만들기`
    - **`선택한 이유: 피어피드백에 대한 정해진 방법은 없지만 피어 피드백 시간이 항상 부족하다고 느꼈고, 생각나는 대로 주제를 꺼내다 종료되었던 적이 많다. 피어 피드백 템플릿을 만들어두면, 피드백 요소 뿐만아니라 칭찬을 통해 긍정 피드백도 잊지 않고 할 수 있다는 점에서 2번을 선택했다.`**
### `J250 조동협`
1. **`선택한 퀘스트`**
    - `Quest 1. 피어세션용 문서를 AI로 채점받기`
    - **`피어 세션하는 이유는 다른 분의 시각에서의 의견을 통해 내 코드를 점검하는 것이다, 이때 발전된 AI를 효율적인 프롬프트를 통해서 점검 받는다면 더 좋은 피어 세션을 통해 같은 팀원들의 코드 발전이 기대되어 이 퀘스트를 선택하였습니다.`**
### `J033 김동완`
1. **`선택한 퀘스트`**
    - `Quest 3. 피어세션 문서와 예상 질문들을 기반으로 보다 예리하고 유의미한 질문 생성해보기`
    - **`선택한 이유: 저는 주로 늦은 시간까지 작업을 하다 보니, 피어세션에서는 집중력이 떨어져 깊이 있는 질문을 잘 만들어내지 못하는 경우가 많았습니다. 이런 아쉬움을 개선하고자, 이번 퀘스트를 통해 AI의 도움을 받아 보다 예리하고 유의미한 질문들을 미리 준비하고자 합니다. 이를 통해 동료와 저 모두에게 도움이 되는, 더 생산적이고 만족스러운 피어세션을 만들고 싶습니다.`**
### `J165_유태근`
1. **`선택한 퀘스트`**
    - `Quest 3. 피어세션 문서와 예상 질문들을 기반으로 보다 예리하고 유의미한 질문 생성해보기`
    - **`선택한 이유: 피어 피드백 전에 내 작업물에 대한 예상 질문을 단편적으로 만들어보는 것만으로는, 질문을 수동적으로 수용하는 데 그쳐 작업물에 대한 본질적인 이해나 실질적인 개선으로 이어지기 어려웠습니다. 그래서 단순히 예상 질문을 받아들이는 데서 나아가, 그 질문을 AI에게 다시 질문하며 나의 코드와 설계를 더 깊이 성찰하고자 이 퀘스트를 선택했습니다.`**

---

# 27조 멤버의 퀘스트 후기

**`J250 조동협`**

1. **`선택한 퀘스트`**
    - `Quest 1. 피어세션용 문서를 AI로 채점받기`
    - 채점 프롬프트를 이용해, 피어세션 문서의 부족한 부분을 AI에게 피드백받고, 피드백 내용을 참고해 보완하고 개선하기
<details>
<summary>사용한 프롬프트 (  J017 피어분의 기존 프롬포트가 좋아서 이를 이용했습니다.)</summary>
    
    ```
    당신은 냉정하고 객관적인 개발 문서 평가자입니다. 감정이나 격려는 배제하고, 오직 사실과 기준에 근거하여 평가하세요. 제공된 문서가 문제 분석부터 구현까지의 전체 개발 과정을 얼마나 잘 기록하고 있는지 평가해주세요.
    
    **평가 원칙:**
    
    - 절대 관대하게 점수를 주지 마세요
    - 애매한 부분은 낮은 점수로 평가하세요
    - 실제로 확인 가능한 내용만 인정하세요
    - 부족한 부분은 구체적으로 지적하세요
    
    **필수 평가 기준:**
    
    **1. 개발 과정의 완전성 (30%)**
    
    - 문제 분석 → 설계 → 구현의 전체 흐름이 명확히 기술되어 있는가?
    - 각 단계별 의사결정 과정과 근거가 제시되어 있는가?
    - 개발 과정에서의 시행착오나 변경사항이 투명하게 기록되어 있는가?
    
    **2. 과정의 연결성과 맥락 (25%)**
    
    - 각 단계 간의 논리적 연결이 자연스러운가?
    - 이전 단계의 결과가 다음 단계에 어떻게 반영되었는지 명확한가?
    - 전체 개발 스토리가 일관성 있게 전개되는가?
    
    **3. 코드 실행 결과 기록의 충실성 (25%)**
    
    - 주요 코드 실행 결과(출력, 로그, 테스트 결과 등)가 포함되어 있는가?
    - 실행 결과에 대한 해석과 분석이 제공되는가?
    - 예상 결과와 실제 결과의 비교 분석이 있는가?
    - 오류 발생 시 디버깅 과정과 해결 과정이 기록되어 있는가?
    
    **4. 기술적 설명의 품질 (20%)**
    
    - 코드와 알고리즘에 대한 이해하기 쉬운 설명
    - 기술적 선택의 근거와 대안 고려사항
    - 성능이나 효율성에 대한 분석
    
    **평가 방법:**
    각 기준을 1-5점으로 평가하고, 다음 형식으로 피드백을 제공해주세요:
    
    **[기준별 상세 평가]**
    
    - 점수와 근거
    - 잘된 점
    - 개선이 필요한 점
    
    **[종합 평가]**
    
    - 전체 점수 (가중평균)
    - 문서의 핵심 강점 2-3개
    - 우선 개선사항 3개
    - 개발 과정 문서로서의 완성도 총평
    ```
</details>
<details>
<summary>결과</summary>
    
    ## [기준별 상세 평가]
    
    ### 1. 개발 과정의 완전성 (30%)
    
    **점수: 2/5** → 가중점수: 0.6
    
    **근거:**
    
    - 문제 요구사항이 문서에 길게 복붙되어 있으나, 그에 대한 **자신만의 분석**이나 우선순위, 범위 정의가 부족하다.
    - 설계 단계(모듈 분리, 데이터 흐름, 책임 분배 등)에 대한 구체적 결정 사항, 왜 그렇게 설계했는지에 대한 근거가 거의 없다.
    - 구현은 일부 존재하지만 “왜 이렇게 구현했는지”, “다른 대안은 무엇이었는지”, “어떤 기준으로 선택했는지”가 기록되어 있지 않아 의사결정의 추적이 불가능하다.
    - 시행착오나 변경 내역(예: 초기 설계와 달리 바뀐 점, 실패했던 접근)은 드러나지 않는다.
    
    **잘된 점:**
    
    - 요구사항(기능, 제한 등)을 문서에 빠짐없이 나열해 두었고, 구현 대상이 무엇인지 명확히 하고 있다.
    - 테스트용 함수 분리(`test_*`)처럼 단위별로 접근하려는 시도가 보인다.
    
    **개선 필요점:**
    
    - 문제를 자신의 말로 요약한 분석 섹션을 추가하고, 핵심 제약(예: 비동기, 단일 스레드 모델)과 그것이 설계에 어떤 영향을 준지 기술할 것.
    - 각 모듈(업로드, 변환, 검증, 대시보드 등)의 책임/인터페이스/경계와 선택한 구조에 대한 이유를 기록.
    - 구현 중 발생한 오류, 시도했던 실패한 접근, 그로 인한 수정 과정을 타임스탬프와 함께 남겨야 한다.
    
    ---
    
    ### 2. 과정의 연결성과 맥락 (25%)
    
    **점수: 2/5** → 가중점수: 0.5
    
    **근거:**
    
    - 요구사항 → 설계 → 구현의 흐름이 “연결된 스토리”로 정리되어 있지 않다. 현재는 요구사항이 있고, 코드가 있고, 그 사이를 잇는 설명이 거의 없다.
    - 예를 들어 “업로드가 들어오면 변환으로, 변환 완료면 검증으로” 라는 흐름은 코드로 일부 표현되지만, 왜 이벤트 기반으로 설계했는지, queue를 누가 어떻게 감시하고 분배하는지 등의 맥락 설명이 빠져 있다.
    - 이전 단계 결과가 다음 단계에 어떻게 반영되었는지가 명시적이지 않다. (예: 변환 모듈 확장/병렬 처리 요구가 있을 때 기존 구현을 어떻게 확장할 예정인지가 없음)
    
    **잘된 점:**
    
    - EventEmitter를 중심으로 이벤트 흐름(예: `upload` → `startDecode` → `startLicense`)을 코드로 일부 구성해 놓았다는 점에서 흐름의 개념은 존재.
    - `Dashboard`가 상태를 받아 렌더링하는 구조로 변환/검증 상태를 연결하려는 의도가 보인다.
    
    **개선 필요점:**
    
    - 각 단계 사이의 “무슨 이벤트가, 어떤 조건에서, 누구에게 전달되는지”를 순서도/플로우 차트로 시각화하고 문서화할 것.
    - 모듈 간의 의존성과 데이터 전달 방식(예: upload payload 구조, 상태 전이)을 글로 기술하여 맥락을 명확히 해야 한다.
    - 확장(예: 변환 모듈 여러 개, 검증 모듈 복수) 요구가 있을 때 기존 단일 구조를 어떻게 진화시킬지에 대한 계획을 연결 지어 설명할 것.
    
    ---
    
    ### 3. 코드 실행 결과 기록의 충실성 (25%)
    
    **점수: 2/5** → 가중점수: 0.5
    
    **근거:**
    
    - 코드에 `console.log`가 많지만 **실제 실행 결과(스크린샷/로그 캡처)**가 문서에 포함되어 있지 않다.
    - 테스트 코드가 존재하나, 테스트 수행 결과(통과/실패 여부, 왜 실패했는지, 기대값 vs 실제값 비교 등)는 기록되어 있지 않다.
    - 비동기 타이머를 제어하기 위해 `jest.useFakeTimers()`와 `jest.runAllTimers()`를 쓴 점은 적절하나, 그에 대한 설명(이런 방식으로 비동기 타이밍을 deterministic하게 만드는 이유)이나 테스트 시나리오 설명, 경계 조건 테스트가 없다.
    - 오류 발생 시의 디버깅 과정(예: 예상했던 상태가 안 나왔을 때 어떤 로그를 찍고 어떻게 원인을 좁혔는지)이 없다.
    
    **잘된 점:**
    
    - 테스트용 분리 함수(`test_upload`, `test_startDecode`, ...)를 만들어 “단위 기능”을 독립적으로 실행하려는 설계가 보인다.
    - Jest의 fake timer를 활용해 비동기 시간 흐름을 통제하려는 점은 테스트 환경 설계에서 적절한 기술 선택이다.
    
    **개선 필요점:**
    
    - 각 테스트 케이스에 대해 “무엇을 검증하려는지”, “입력은 무엇이고 기대 출력은 무엇인지”를 명시하고, 실제 실행 로그와 비교해서 문서화할 것.
    - 변환/검증 흐름 관련 정상/비정상 시나리오를 만들고, 그 결과를 기록 (예: 큐에 없을 때, 동시성 충돌, 대기열 처리 순서 등).
    - 실패한 테스트나 예상과 다른 동작이 나왔을 경우 “문제 발견 → 원인 규명 → 해결” 단계를 시계열로 정리할 것.
    
    ---
    
    ### 4. 기술적 설명의 품질 (20%)
    
    **점수: 2/5** → 가중점수: 0.4
    
    **근거:**
    
    - 코드에 주석이 일부 있으나 “왜 이 구조를 썼는가”, “다른 대안은 무엇인가”, “이벤트 기반을 쓴 이유”, “동시성/병렬성을 어떻게 다루려 하는가” 등에 대한 깊이 있는 설명은 거의 없다.
    - 성능이나 확장성(예: 한 번에 하나만 처리하는 변환/검증 모듈에서 다수로 확장할 때 병목이 어디인지, 큐 대기 시간 증가시 어떤 전략을 쓸지)에 대한 언급 없음.
    - 에러 처리/예외 조건에 대한 설명이나 정책이 문서에 없다.
    
    **잘된 점:**
    
    - 모듈 이름과 책임(예: Dashboard, License 검증 등)은 코드로 나뉘어 있고, 그 의도는 부분적으로 드러난다.
    
    **개선 필요점:**
    
    - 핵심 기술 선택(이벤트 에미터 vs 다른 패턴, setTimeout으로 시뮬레이션한 이유, 테스트에서 fake timer 사용 이유)을 문서에 명시하고, 기대 효과와 trade-off를 비교 설명할 것.
    - 각 모듈의 시간 복잡도/병목 가능성을 평가하고, 확장 시 예상되는 문제를 서술할 것.
    - 실패 조건과 복구 전략 (예: 변환 실패시 재시도, 검증 모듈 점유 문제) 을 디자인 문서에 포함할 것.
    
    ---
    
    ## [종합 평가]
    
    - **전체 점수 (가중평균):**
        
        0.6+0.5+0.5+0.4=2.00.6 + 0.5 + 0.5 + 0.4 = 2.00.6+0.5+0.5+0.4=2.0 / 5 → **40점 / 100점** 수준
        
    - **문서의 핵심 강점:**
        1. 요구사항을 빠짐없이 정리해 두어 “무엇을 해야 하는지”는 명확하게 캡쳐되어 있다.
        2. EventEmitter 기반 흐름과 대시보드/모듈 분리를 통해 비동기 파이프라인 구조를 처음부터 염두에 둔 구현 시도가 존재한다.
        3. 테스트를 위해 fake timer를 활용한 구조를 분리해 둔 점은 비동기 환경에서 검증 가능하게 만들려는 의도다.
    - **우선 개선사항 3개:**
        1. **설계 문서 보강:** 분석 → 설계 → 구현으로 이어지는 흐름을 글과 다이어그램(플로우, 책임 분배, 모듈 인터페이스)으로 명확히 기록하라.
        2. **실행 결과 기록:** 테스트 및 실행 로그를 캡처하고, 예상 vs 실제를 비교한 해석을 포함하여 “무슨 일이 언제 일어났는지”를 시계열로 정리하라.
        3. **기술적 이유 설명:** 왜 EventEmitter 구조를 택했는지, 비동기 처리 모델의 장단점, 확장 시 고려사항 등을 명시적으로 기술하여 향후 유지보수자나 자신이 되돌아볼 때 맥락을 제공하라.
    - **개발 과정 문서로서의 완성도 총평:**
        
        현재 문서는 “무엇을 해야 하는지”에 대한 요구사항 수집과 일부 기능 구현 코드가 혼재된 상태로, 개발 과정 문서로서의 일관된 서사나 추적 가능성, 이유 설명이 부족하다. 구조적 기록(설계 결정, 흐름도, 테스트 결과 해석 등)과 맥락 연결이 더해지면 학습용/협업용 문서로서 기본을 갖출 수 있으나, 지금 상태로는 재현성, 판단의 근거, 확장성 검토가 어렵다. 현 수준은 초안 단계이며, 다음 리비전을 통해 “의도 → 선택 → 결과 → 피드백”의 순환이 명확하게 드러나는 구조로 재정비해야 한다.
</details>

- **설계 문서 보강:** 분석 → 설계 → 구현으로 이어지는 흐름을 글과 다이어그램(플로우, 책임 분배, 모듈 인터페이스)으로 명확히 기록하라.
- **실행 결과 기록:** 테스트 및 실행 로그를 캡처하고, 예상 vs 실제를 비교한 해석을 포함하여 “무슨 일이 언제 일어났는지”를 시계열로 정리하라.
- **기술적 이유 설명:** 왜 EventEmitter 구조를 택했는지, 비동기 처리 모델의 장단점, 확장 시 고려사항 등을 명시적으로 기술하여 향후 유지보수자나 자신이 되돌아볼 때 맥락을 제공하라.
- 후기 및 느낀점
    - 내가 부족하다고 느꼈던 점이다. 단순히 오늘만이 아닌 챌린지 동안 많이 보완하고 싶었던 점인데 이를 정확하게 지적해주니 AI를 통한 채점 피드백도 프롬프트를 잘 작성한다면 정말 도움이 된다고 느꼈다.
    - 이러한 문제점은 문제의 의도를 정확히 파악하지 못하고 학습을 진행하니 문제에 대한 풀이가 엇나가고 학습이 미흡하여 해석이 미흡한 것으로 보인다. 따라서 정확한 문제 파악과 학습 진행에대한 설계를 세우고 이를 바탕으로 체크 포인트를 구체화하여 작성해봐야겠다.
    - +) 다음엔 2.5/5 점은 넘겨보자 .. 😂
 
### **`J165 유태근`**
1. **선택한 퀘스트**
    - `Quest 3`. 피어세션 문서와 예상 질문들을 기반으로 보다 예리하고 유의미한 질문 생성해보기
    - 단순히 묻고 답변을 수동적으로 받기만 하는 것이 아니라, AI가 먼저 질문을 제시하면 내가 답변하고, 다시 그 답변에 대해 AI에게 추가 질문을 던지며 깊이 있는 대화를 이어가보자.

2. **수행 기록**
    <details>
    <summary>1일차 [2025-07-28]</summary>
    <h3>▶️ 진행</h3>

    - AI에게 "비동기 처리 문법, 병렬처리, 스레드, 이벤트 큐, Promise, EventEmitter 등 이 주제에 대해 피어세션에서 나올만한 질문들을 알려줘"라고 요청함.

    - AI가 제시한 질문 리스트에 대해 하나씩 답변하고, AI로부터 답변에 대한 피드백과 보완 설명을 받으며 학습함.

    <h3>✅ 결과</h3>

    AI가 제시한 주요 질문 예시:

        “비동기 처리와 동기 처리의 차이는 무엇인가요?”

        “Promise가 무엇이고, 어떻게 활용되나요?”

        “EventEmitter의 역할과 동작 원리는 무엇인가요?”

    내가 "Promise는 비동기 작업의 결과를 나타내는 객체로, then과 catch를 통해 후속 처리를 연결할 수 있습니다"라고 답변하자,

    AI 피드백:
    <blockquote>정확한 설명이며입니다. 추가로 Promise 상태(state)와 체이닝 개념도 이해하면 좋습니다</blockquote>
    <h3>💡 회고</h3>
    질문을 먼저 받고 답변하며 피드백을 받으니, 단순 암기가 아닌 개념의 핵심과 세부를 동시에 잡을 수 있어 효과적인 학습이었다. 앞으로도 이런 방식으로 준비하면 피어세션에서 더 자신감을 가질 수 있을 것 같다.
    </details>

    ---

    <details>
    <summary>2일차 [2025-07-29]</summary>
    <h3>▶️ 진행</h3>

    - AI에게 "비동기 시뮬레이션을 테스트 가능한 구조로 개선할 때 어떤 질문이 나올 수 있을까?"를 요청하여 예상 질문 리스트를 받음.

    - 각 질문에 직접 답변하고, AI에게 피드백을 받아 개념을 정교화함.
    <h3>✅ 결과</h3>

    AI가 제시한 주요 질문 예시:

        “시뮬레이션 코드를 테스트하기 좋은 구조란 어떤 조건을 갖추어야 하나요?”

        “setTimeout 기반의 로직은 어떻게 테스트 가능하게 리팩터링할 수 있나요?”

        “EventEmitter 기반 로직을 테스트하려면 어떤 전략이 필요한가요?”

    내가 "테스트가 쉬운 구조란 외부 상태에 의존하지 않고, 입출력이 명확하게 드러나며, 단위로 분리된 구조라고 생각합니다"라고 답변하자,

    AI 피드백:
    <blockquote>“정확합니다. 특히 비동기 로직은 콜백보다는 Promise나 async/await를 사용하면 테스트할 때 결과를 예측하고 검증하기 훨씬 수월합니다.”</blockquote>

    <h3>💡 회고</h3>
    평소엔 기능만 작동하면 된다고 생각했는데, 테스트가 가능한 구조로 바꾸는 과정에서 코드의 재사용성과 안정성까지 챙길 수 있다는 걸 배웠다. 질문을 받고 직접 답해보니 내가 놓치고 있던 관점을 되짚는 데 큰 도움이 됐다.
    </details>

    ---

    <details>
    <summary>3일차 [2025-07-30]</summary>
    <h3>▶️ 진행</h3>

    - AI에게 "Git과 호환 가능한 커스텀 도구를 만들기 위해 add, commit, log 명령어를 구현할 때 어떤 질문이 나올 수 있을까?"를 요청하여 예상 질문 리스트를 받음.

    - 각 질문에 대해 스스로 답변한 후, AI로부터 피드백과 보완 설명을 받으며 구현 방향과 설계 개념을 정립함.

    <h3>✅ 결과</h3>

    AI가 제시한 주요 질문 예시:

        “Git에서 add 명령어는 어떤 내부 동작을 통해 index에 파일을 등록하나요?”

        “커밋 객체는 어떤 정보들로 구성되며, 이를 파일 시스템에 어떻게 저장하나요?”

        “커스텀 도구에서 log 명령을 구현하려면 어떤 데이터 흐름을 따라야 하나요?”

    내가 "add는 working directory에서 변경된 파일을 해시한 뒤 .git/index에 등록하는 명령입니다"라고 답변하자,

    AI 피드백:
    <blockquote>
    “핵심을 잘 짚었습니다. 추가로 .git/objects에 Blob 객체를 저장하고, index는 해당 경로를 추적하는 중간 단계임을 언급하면 더 명확해집니다.”
    </blockquote>

    또 log 구현 시 "단순히 commit 메시지를 나열하는 것이 아니라, commit 객체가 부모 SHA를 어떻게 참조하는지를 기반으로 트리 구조를 따라가야 한다"고 피드백 받음.

    <h3>💡 회고</h3>
    </details>

    ---

    <details>
    <summary>4일차 [2025-07-31]</summary>
    <h3>▶️ 진행</h3>
    
    - 전날 만든 커스텀 Git 도구의 add, commit, log 기능을 테스트하며 동작을 검증하고,

    - AI에게 "Git의 clone 명령어를 직접 구현할 때 나올 수 있는 질문들을 알려줘"라고 요청하여 예상 질문을 수집함.

    - 받은 질문에 스스로 답변한 뒤, AI로부터 피드백을 받아 구현 흐름을 구체화함.

    <h3>✅ 결과</h3>

    AI가 제시한 주요 질문 예시:

        “Git의 clone 명령어는 내부적으로 어떤 순서로 동작하나요?”

        “.git 디렉토리의 어떤 구조를 복제해야 진짜 ‘clone’이 되는가?”

        “커밋과 객체 간의 연결관계를 복제할 때 주의할 점은 무엇인가요?”

    내가 "clone은 .git/objects, .git/refs, .git/HEAD 등을 복사하고, working directory도 checkout하여 초기화하는 흐름입니다"라고 답변하자,

    AI 피드백:
    <blockquote>“좋은 설명입니다. 단순 복사와 달리, 커밋 트리의 참조 구조를 올바르게 복원해야 이후 log, commit이 정상 작동합니다. 특히 .git/refs/heads/master와 HEAD 파일이 가리키는 경로가 정확해야 합니다.”</blockquote>

    또 clone 구현 후 테스트 시, "index나 HEAD 정보가 불완전할 경우, add나 commit 명령어가 오작동할 수 있다"는 점도 피드백받아 관련 부분 보완함.
    <h3>💡 회고</h3>
    단순히 기능을 구현하는 걸 넘어서, 실제 Git의 구조와 흐름을 고려하며 복제 기능을 구현해보니 설계 관점에서 많은 인사이트를 얻었다. 테스트하면서 실제 Git처럼 동작하게 만드는 것이 얼마나 세밀한 작업인지 체감했고, 단일 기능이 아닌 시스템 단위로 바라보는 습관이 길러졌다.
    </details>

    ---

    <details>
    <summary>5일차 [2025-08-01]</summary>
    <h3>▶️ 진행</h3>
    - 최근 4일간 수행한 두 미션:

        - 비동기 시뮬레이션 구조 구현 + 테스트 최적화 개선

        - Git 객체 기반 도구 개발 + 클론 명령어 확장

    - 각 미션을 더 효과적으로 학습하기 위해 어떤 전략이 좋았을지 AI에게 질문 리스트를 먼저 요청하고,
    내 스스로 답변한 뒤 AI 피드백을 받아 학습 방향을 되돌아봄.
    <h3>✅ 결과</h3>
    AI가 제안한 대표 질문들:

        “비동기 시뮬레이터의 구조를 설계할 때, 동기/비동기/지연의 차이를 어떤 테스트 기준으로 설계했나요?”

        “시뮬레이션 코드를 테스트하기 좋게 만들기 위해 어떤 리팩터링을 했고, 다른 대안은 없었나요?”

        “Git 도구를 만들면서 실제 Git의 내부 흐름과 내가 만든 구현이 달랐던 순간은 언제였나요?”

    예시 응답 & 피드백:

    `내 답변`: “비동기 이벤트 버스는 실제 동시성 상황과 달라서 타이밍 제어가 애매했어요. 구조를 나누긴 했는데, 테스트가 제일 어렵더라고요.”

    AI 피드백:

    <blockquote>“정확해요. 테스트 가능한 구조를 먼저 고민한 후 아키텍처를 구성하는 연습이 비동기 미션의 핵심이에요. 타이밍 의존 코드는 event log나 state snapshot을 통해 검증 가능하게 리팩터링하는 접근이 좋았을 거예요.”</blockquote>
    </br>
    `내 답변`: “Git 도구는 tree-object 흐름을 정확히 이해했으면 커밋 구현이 덜 막혔을 것 같아요.”

    AI 피드백:

    <blockquote>“맞아요. 실제 Git 명령어 실행 후 .git 내부 구조 변화를 추적하며 ‘예측 → 관찰 → 비교’를 반복했다면 더 빠르게 구조화할 수 있었을 거예요.”</blockquote>

    <h3>💡 회고</h3>
    이번 4일간의 미션을 돌아보며, 복잡한 구조를 구현하는 데 있어 **먼저 관찰하고, 예측하고, 나의 구조를 비교하는 연습** 이 학습 효과를 크게 높인다는 걸 느꼈다. 앞으로는 단순 구현보다 `“예측 → 구현 → 검증”` 루틴을 기반으로 학습을 설계해가야겠다고 배웠다.
    </details>

3. **느낌점**
    - 퀘스트를 진행하면서 단순히 질문을 받는 데서 벗어나 스스로 답변하고, 다시 AI에게 추가 질문을 던지는 과정이 생각보다 훨씬 깊이 있는 학습이 되었다. 
    - 이렇게 능동적으로 대화를 주도하니 코드와 설계에 대해 더 명확하게 이해하고, 부족한 부분도 빠르게 발견할 수 있었다. 😁

**S001 강윤서**

 
<details>

<summary>AI를 사용할 때 역할을 부여하면 더 명확하게 응답을 해주는 것 같아 프롬프팅을 진행했다.</summary>

```swift
목표: 동료의 학습 저장소를 제한된 시간에 꼼꼼히 보기
행동: 동료의 학습 저장소를 꼼곰히 보기 위한 템플릿 작성(맞춤형 질문이 있다면 더 좋아)
AI 역할: 내가 피드백을 잘 할 수 있도록 도와주는 문서 작성자
내 역할: 피드백 하는 사람

이걸 기반으로 피어세션의 피드백 템플릿을 만들어줘
gist 링크: @https://~
```

</details>

<details>

<summary>바탕으로 만들어진 템플릿</summary>

```markdown
# ⚡ 학습 저장소 빠른 리뷰 템플릿

> **목표**: 제한된 시간(10-20분)에 핵심만 파악하여 의미 있는 피드백 제공

## ⏰ **시간별 리뷰 가이드**

### 🚀 **10분 버전** (핵심만)
```
2분 - README & 목표 파악
5분 - 핵심 코드 3-4개 파일만 집중
2분 - 가장 인상적인 부분 + 개선점 1개
1분 - 질문거리 생각
```

### 📋 **15분 버전** (표준)
```
3분 - 전체 구조 + 문서 파악
8분 - 코드 품질 + 설계 분석
3분 - 학습 과정 + 피드백 정리
1분 - 대화 주제 준비
```

### 🔍 **20분 버전** (꼼꼼히)
```
5분 - 전체적 이해
10분 - 코드 깊이 분석
3분 - 학습 과정 파악
2분 - 종합 피드백 정리
```

---

## 📝 **빠른 체크 시트**

### 🎯 **1분만에 파악하기**
- [ ] **목표**: 뭘 만들려고 했나? _(README 첫 줄)_
- [ ] **완성도**: 목표 대비 얼마나 구현했나? _(폴더/파일 개수)_
- [ ] **기술**: 어떤 스택 사용했나? _(확장자/import문)_

### ⚡ **5분 코드 스캔**
- [ ] **메인 로직**: 핵심 기능이 어떻게 구현되었나?
- [ ] **구조**: 파일들이 논리적으로 분리되었나?
- [ ] **가독성**: 함수/변수명이 명확한가?
- [ ] **특이점**: 독특하거나 인상적인 구현이 있나?

### 📚 **3분 학습 체크**
- [ ] **기록**: 배운 것을 어떻게 정리했나?
- [ ] **시행착오**: 문제 해결 과정이 보이나?
- [ ] **성장**: 초기 대비 개선된 부분은?

---

## 💡 **맞춤형 핵심 질문들**

### 🔍 **코드 리뷰 질문** (파일별)
```
Q: 이 파일의 핵심 역할은?
Q: 가장 복잡한 로직은 어디? 왜?
Q: 다른 파일과 어떻게 연결되나?
Q: 개선할 점이 보인다면?
```

### 🎯 **설계 이해 질문**
```
Q: 왜 이런 구조로 만들었을까?
Q: 확장성은 어떨까?
Q: 재사용 가능한 부분은?
Q: 가장 잘 설계된 부분은?
```

### 📖 **학습 과정 질문**
```
Q: 어떤 기술적 도전이 있었을까?
Q: 예상과 달랐던 부분은?
Q: 새롭게 배운 개념은?
Q: 다음에 적용할 수 있는 것은?
```

---

## 🌟 **빠른 피드백 양식**

### ✨ **칭찬 포인트** (1-2개만 집중)
**가장 인상적인 점:**
- **기술적**: 
- **학습 태도**: 

### 🚀 **개선 제안** (1개만 핵심)
**우선순위 1:**
- **현재**: 
- **제안**: 
- **이유**: 

### 🤔 **대화 주제** (1-2개)
1. 
2. 

---

## 📊 **리뷰 결과 요약**

### 🎯 **종합 평가** (한 문장)
```
이 프로젝트의 핵심은 _______이고, 
가장 인상적인 부분은 _______이다.
```

### 💪 **성장 포인트** (한 문장)
```
_______ 부분에서 명확한 성장이 보이며,
다음엔 _______를 도전해보면 좋겠다.
```

### 🔥 **대화하고 싶은 주제** (한 문장)
```
_______ 부분이 궁금해서 
구현 과정을 더 자세히 들어보고 싶다.
```

---

## 🎨 **리뷰 스타일별 접근법**

### 🔍 **디테일형** (꼼꼼히 보는 스타일)
- 코드 한 줄 한 줄 정확성 체크
- 에러 처리, 엣지 케이스 확인
- 성능, 메모리 효율성 고려

### 🎯 **구조형** (큰 그림 보는 스타일)
- 전체 아키텍처 패턴 파악
- 모듈간 의존성 관계 분석
- 확장성, 유지보수성 평가

### 📚 **학습형** (교육 중심 스타일)
- 학습 목표 달성도 체크
- 새로운 개념 적용 여부
- 지식 체화 과정 관찰

### 🚀 **실용형** (실무 관점 스타일)
- 실제 사용 가능성 평가
- 협업 시 고려사항 체크
- 운영 환경 적용 가능성

---

## ⚡ **시간 단축 팁**

### 📂 **효율적인 파일 탐색**
1. **README** → **main/index 파일** → **핵심 로직 파일**
2. **가장 큰 파일 3개** 우선 확인
3. **테스트 파일**로 의도 파악
4. **커밋 메시지**로 개발 과정 추적

### 🔍 **빠른 코드 스캔 방법**
- **함수명만** 먼저 쭉 훑기
- **주석/TODO** 찾아서 의도 파악
- **import문**으로 사용 기술 체크
- **매직넘버/상수**로 비즈니스 로직 이해

### 💬 **효과적인 질문 만들기**
- "좋다/나쁘다" → "왜 이렇게?"
- "틀렸다" → "다른 방법은?"
- "어렵다" → "어떤 부분이?"
- "간단하다" → "어떻게 단순화했나?"

---

**⏰ 시간 부족할 때 우선순위**
1. **목표 이해** (꼭 필요)
2. **핵심 로직** (가장 중요)
3. **인상적인 점** (칭찬거리)
4. **개선점 1개** (도움되는 것)
5. **질문거리** (대화 연결)
```

</details>

<details>

<summary>결과</summary>

[피어 피드백](https://www.notion.so/24051bd903fe803e9becdd58cb475903?pvs=21) 

</details>

<details>

<summary>느낀점</summary>

전에는 체계적이지 않게 생각나는대로 질문을 했다면, 이번에는 피드백의 흐름을 좀 생각할 수 있었다.

그치만 아침 시간에 템플릿을 다 채울 수 있을정도로 꼼꼼히 보지는 못했다.

대신 템플릿에서 제안해준 시간 단축 팁과 리뷰 스타일별 접근은 제법 유용했다.

특히 개선 미션을 하는 날에는 직접 git을 clone해서 커밋 메세지 시점으로 돌아가 어떻게 코드를 수정했는지 좀 더 편하게 볼 수 있었다.

그리고 이번주 피드백에서는 칭찬도 꼭 잊지 않으려고 했다.

</details>
